{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "classifier_meta.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qlD9J59fEP9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "faaf4a04-dfb9-478c-de08-4a07401995e9"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Jun 14 11:07:41 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.36.06    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vab0BovRB0YG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        },
        "outputId": "d8be89ab-760c-4b5b-dd27-c0403370e209"
      },
      "source": [
        "!pip install tensorboardx\n",
        "!mkdir checkpoints\n",
        "!mkdir results\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorboardx\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/f1/5843425495765c8c2dd0784a851a93ef204d314fc87bcc2bbb9f662a3ad1/tensorboardX-2.0-py2.py3-none-any.whl (195kB)\n",
            "\r\u001b[K     |█▊                              | 10kB 16.6MB/s eta 0:00:01\r\u001b[K     |███▍                            | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |█████                           | 30kB 2.2MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 40kB 2.5MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |██████████                      | 61kB 2.3MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 71kB 2.5MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 81kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 92kB 3.0MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 112kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 122kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 133kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 143kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 153kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 163kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 174kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 184kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 194kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 204kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardx) (1.18.5)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardx) (3.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardx) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardx) (47.1.1)\n",
            "Installing collected packages: tensorboardx\n",
            "Successfully installed tensorboardx-2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D20RV0WiluRk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "2a985b14-37c1-4add-f48c-c40a92e1408c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xfhaLJNokrL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import BatchSampler\n",
        "from itertools import cycle\n",
        "from tensorboardX import SummaryWriter\n",
        "import torch.autograd as autograd\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5oOx-spouSY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class BalancedBatchSampler(BatchSampler):\n",
        "    \"\"\"\n",
        "    BatchSampler - from a MNIST-like dataset, samples n_classes and within these classes samples n_samples.\n",
        "    Returns batches of size n_classes * n_samples\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset, n_classes, n_samples,custom_classes =None):\n",
        "        loader = DataLoader(dataset)\n",
        "        self.labels_list = []\n",
        "        self.custom_classes = custom_classes\n",
        "        for _, label in loader:\n",
        "            self.labels_list.append(label)\n",
        "        self.labels = torch.LongTensor(self.labels_list)\n",
        "        self.labels_set = list(set(self.labels.numpy()))\n",
        "        self.label_to_indices = {label: np.where(self.labels.numpy() == label)[0]\n",
        "                                 for label in self.labels_set}\n",
        "        for l in self.labels_set:\n",
        "            np.random.shuffle(self.label_to_indices[l])\n",
        "        self.used_label_indices_count = {label: 0 for label in self.labels_set}\n",
        "\n",
        "        self.count = 0\n",
        "        self.n_classes = n_classes\n",
        "        self.n_samples = n_samples\n",
        "        self.dataset = dataset\n",
        "        self.batch_size = self.n_samples * self.n_classes\n",
        "\n",
        "    def __iter__(self):\n",
        "        self.count = 0\n",
        "        while self.count + self.batch_size < len(self.dataset):\n",
        "            classes = np.random.choice(self.labels_set, self.n_classes, replace=False)\n",
        "            indices = []\n",
        "            if self.custom_classes!=None:\n",
        "                classes = self.custom_classes\n",
        "            for class_ in classes:\n",
        "                indices.extend(self.label_to_indices[class_][\n",
        "                               self.used_label_indices_count[class_]:self.used_label_indices_count[\n",
        "                                                                         class_] + self.n_samples])\n",
        "                self.used_label_indices_count[class_] += self.n_samples\n",
        "                if self.used_label_indices_count[class_] + self.n_samples > len(self.label_to_indices[class_]):\n",
        "                    np.random.shuffle(self.label_to_indices[class_])\n",
        "                    self.used_label_indices_count[class_] = 0\n",
        "            yield indices\n",
        "            self.count += self.n_classes * self.n_samples\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset) // self.batch_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQwxVyXnoz3x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "writer = SummaryWriter('runs_gan/correctedmetaclassifier_wgan_mnist')\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "img_size = 28\n",
        "##Simulataneous training of different classes\n",
        "n_classes = 1\n",
        "#custom_classes = np.random.randint(0,10,n_classes)\n",
        "custom_classes = np.array([9])\n",
        "#custom_classes = None\n",
        "class_batch = 64\n",
        "transform = transforms.Compose([torchvision.transforms.Resize(img_size),torchvision.transforms.ToTensor()])\n",
        "dataset = torchvision.datasets.MNIST(root=\"./\", train=True,download=True,transform=transform)\n",
        "#balanced_batch_sampler = BalancedBatchSampler(dataset, n_classes, class_batch,custom_classes)\n",
        "batch_size = 64\n",
        "\n",
        "#dataloader = torch.utils.data.DataLoader(dataset, batch_sampler=balanced_batch_sampler)\n",
        "class_datasets = [torch.utils.data.dataset.Subset(dataset, np.where(dataset.targets==i)[0]) for i in range(10)]\n",
        "dataloaders = [torch.utils.data.DataLoader(class_datasets[i],batch_size=batch_size,shuffle=True,num_workers=4) for i in range(10)]\n",
        "##parameters\n",
        "\n",
        "dim = 64\n",
        "LAMBDA = 10\n",
        "epochs= 20000\n",
        "n_z = 128\n",
        "k = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caSS0IjEpA2K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "182a4f35-e541-4f34-e4fb-3e894103b587"
      },
      "source": [
        "\n",
        "\n",
        "class netD(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(netD,self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1,dim,5,stride=2,padding=2)\n",
        "        self.act1 = nn.ReLU(True)\n",
        "        self.conv2 = nn.Conv2d(dim,dim*2,5,stride=2,padding=2)\n",
        "        self.act2 = nn.ReLU(True)\n",
        "        self.conv3 = nn.Conv2d(dim*2,dim*4,5,stride=2,padding=2)\n",
        "        self.act3 = nn.ReLU(True)\n",
        "        self.fc1 = nn.Linear(4*4*4*dim,1)\n",
        "\n",
        "    def forward(self,input):\n",
        "        input = input.view(-1,1,28,28)\n",
        "\n",
        "        l1_1 = self.conv1(input)\n",
        "        l1_2 = self.act1(l1_1)\n",
        "        l2_1 = self.conv2(l1_2)\n",
        "        l2_2 = self.act2(l2_1)\n",
        "        l3_1 = self.conv3(l2_2)\n",
        "        l3_2 = self.act3(l3_1)\n",
        "\n",
        "        l4_1 = self.fc1(l3_2.view(-1,4*4*4*dim))\n",
        "        return l4_1.view(-1)\n",
        "\n",
        "\n",
        "class netG(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(netG,self).__init__()\n",
        "        self.convt1 = nn.Linear(128,4*4*4*dim)\n",
        "        self.act1 = nn.ReLU(True)\n",
        "        self.convt2 = nn.ConvTranspose2d(4*dim,dim*2,5)\n",
        "        self.act2 = nn.ReLU(True)\n",
        "        self.convt3 = nn.ConvTranspose2d(dim*2,dim,5)\n",
        "        self.act3 = nn.ReLU(True)\n",
        "        self.convt4 = nn.ConvTranspose2d(dim,1,8,stride=2)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self,input):\n",
        "        #input = input.view(-1,1,28,28)\n",
        "        l1_1 = self.convt1(input)\n",
        "        l1_2 = self.act1(l1_1)\n",
        "        l1_2 = l1_2.view(-1,4*dim,4,4)\n",
        "        l2_1 = self.convt2(l1_2)\n",
        "        l2_2 = self.act2(l2_1)\n",
        "        l2_2 = l2_2[:,:,:7,:7]\n",
        "        l3_1 = self.convt3(l2_2)\n",
        "        l3_2 = self.act3(l3_1)\n",
        "        l4_1 = self.convt4(l3_2)\n",
        "        output = self.sigmoid(l4_1)\n",
        "        return output.view(-1,img_size*img_size)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZ1_hQr8pFcx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "fixed_batch = torch.rand(class_batch,128,device=device,requires_grad=False)\n",
        "\n",
        "def calc_gradient_penalty(netD, real_data, fake_data):\n",
        "    #print real_data.size()\n",
        "    alpha = torch.rand(class_batch, 1).to(device)\n",
        "    alpha = alpha.expand(real_data.size())\n",
        "    interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n",
        "\n",
        "    interpolates = autograd.Variable(interpolates, requires_grad=True)\n",
        "\n",
        "    disc_interpolates = netD(interpolates)\n",
        "\n",
        "    gradients = autograd.grad(outputs=disc_interpolates, inputs=interpolates,\n",
        "                              grad_outputs=torch.ones(disc_interpolates.size()).to(device),\n",
        "                              create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
        "\n",
        "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * LAMBDA\n",
        "    return gradient_penalty\n",
        "#criterion = nn.BCEWithLogitsLoss()\n",
        "#metanet = netD().to(device)\n",
        "#tasknet = netD().to(device)\n",
        "#taskoptim = optim.Adam(tasknet.parameters(),lr=0.0001)\n",
        "#metaoptim = optim.Adam(metanet.parameters(),lr=0.00001)\n",
        "\n",
        "##training\n",
        "\n",
        "metadiscriminator = netD().to(device)\n",
        "discriminator = netD().to(device)\n",
        "optimizerD = optim.Adam(discriminator.parameters(), lr=1e-4, betas=(0.5, 0.9))\n",
        "metaoptimizerD = optim.Adam(metadiscriminator.parameters(), lr=1e-5)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4q7tE5DA6NB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "outputId": "246419ab-c681-49ee-db57-f3c3b459dda5"
      },
      "source": [
        "discriminator.load_state_dict(torch.load(\"./checkpoints/run2_gan_discriminator_180009\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-27d36c944b3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./checkpoints/run2_gan_discriminator_180009\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    582\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 584\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    585\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './checkpoints/run2_gan_discriminator_180009'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNcAgScleKjh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "iters=0\n",
        "for epoch in range(epochs):\n",
        "  index1 = np.random.randint(0,10)\n",
        "  index2 = np.random.randint(0,10)\n",
        "  while index1==index2:\n",
        "    index2 = np.random.randint(0,10)\n",
        "  \n",
        "  data_iterator = iter(dataloaders[index1])\n",
        "  fake_iterator = iter(dataloaders[index2])\n",
        "  index = 0\n",
        "  metaoptimizerD.zero_grad()\n",
        "  discriminator.load_state_dict(metadiscriminator.state_dict())\n",
        "  for _ in range(k):\n",
        "    #print(\"cola\")\n",
        "    discriminator.zero_grad()\n",
        "    x_real = data_iterator.next()[0].to(device)\n",
        "    x_real = x_real.view(-1,img_size*img_size)\n",
        "    b_size = x_real.shape[0]\n",
        "    output = discriminator(x_real).view(-1)\n",
        "    D_real = output.mean()\n",
        "\n",
        "    x_fake = fake_iterator.next()[0].to(device)\n",
        "    fake_data = x_real.view(-1,img_size*img_size)\n",
        "    output = discriminator(fake_data).view(-1)\n",
        "    D_fake = output.mean()\n",
        "    D_loss = D_fake - D_real\n",
        "    D_loss.backward()\n",
        "    gradient_p = calc_gradient_penalty(discriminator,x_real.detach(),fake_data.detach())\n",
        "    gradient_p.backward()\n",
        "    total_D_loss = D_loss + gradient_p\n",
        "    optimizerD.step()\n",
        "    writer.add_scalar('Discriminator_Loss', total_D_loss,iters) \n",
        "  for p,metap in zip(discriminator.parameters(),metadiscriminator.parameters()):\n",
        "    diff = metap-p\n",
        "    metap.grad = diff\n",
        "  metaoptimizerD.step()\n",
        "  if iters%200==0:\n",
        "    print(\"Epoch : \"+str(epoch)+\" D_Loss: \" + str(total_D_loss.item()))\n",
        "  if epoch%1000==0:\n",
        "    torch.save(discriminator.state_dict(),'./checkpoints/run4_gan_discriminator_'+str(iters))\n",
        "  iters = iters+1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mca9t3bUsTj6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622
        },
        "outputId": "5cbc58ee-3041-4509-d7e2-ec122cc5f4b8"
      },
      "source": [
        "\n",
        "!zip -r correctedmetaclassifiermnist_result.zip ./results/run*\n",
        "!zip -r correctedmetaclassifiermnist_checkpoint.zip ./checkpoints/run4*\n",
        "!zip -r correctedmetaclassifiermnist_log.zip ./runs_gan"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tzip warning: name not matched: ./results/run*\n",
            "\n",
            "zip error: Nothing to do! (try: zip -r correctedmetaclassifiermnist_result.zip . -i ./results/run*)\n",
            "  adding: checkpoints/run4_gan_discriminator_0 (deflated 8%)\n",
            "  adding: checkpoints/run4_gan_discriminator_1000 (deflated 8%)\n",
            "  adding: checkpoints/run4_gan_discriminator_10000 (deflated 8%)\n",
            "  adding: checkpoints/run4_gan_discriminator_11000 (deflated 8%)\n",
            "  adding: checkpoints/run4_gan_discriminator_12000 (deflated 8%)\n",
            "  adding: checkpoints/run4_gan_discriminator_13000 (deflated 8%)\n",
            "  adding: checkpoints/run4_gan_discriminator_14000 (deflated 8%)\n",
            "  adding: checkpoints/run4_gan_discriminator_15000 (deflated 8%)\n",
            "  adding: checkpoints/run4_gan_discriminator_16000 (deflated 8%)\n",
            "  adding: checkpoints/run4_gan_discriminator_17000 (deflated 8%)\n",
            "  adding: checkpoints/run4_gan_discriminator_18296 (deflated 8%)\n",
            "  adding: checkpoints/run4_gan_discriminator_19296 (deflated 8%)\n",
            "  adding: checkpoints/run4_gan_discriminator_2000 (deflated 8%)\n",
            "  adding: checkpoints/run4_gan_discriminator_20296 (deflated 8%)\n",
            "  adding: checkpoints/run4_gan_discriminator_21296 (deflated 8%)\n",
            "  adding: checkpoints/run4_gan_discriminator_22296 (deflated 8%)\n",
            "  adding: checkpoints/run4_gan_discriminator_23296 (deflated 8%)\n",
            "  adding: checkpoints/run4_gan_discriminator_24296 (deflated 8%)\n",
            "  adding: checkpoints/run4_gan_discriminator_25296 (deflated 8%)\n",
            "  adding: checkpoints/run4_gan_discriminator_26296 (deflated 8%)\n",
            "  adding: checkpoints/run4_gan_discriminator_27296 (deflated 8%)\n",
            "  adding: checkpoints/run4_gan_discriminator_3000 (deflated 8%)\n",
            "  adding: checkpoints/run4_gan_discriminator_4000 (deflated 8%)\n",
            "  adding: checkpoints/run4_gan_discriminator_5000 (deflated 8%)\n",
            "  adding: checkpoints/run4_gan_discriminator_6000 (deflated 8%)\n",
            "  adding: checkpoints/run4_gan_discriminator_7000 (deflated 8%)\n",
            "  adding: checkpoints/run4_gan_discriminator_8000 (deflated 8%)\n",
            "  adding: checkpoints/run4_gan_discriminator_9000 (deflated 8%)\n",
            "  adding: runs_gan/ (stored 0%)\n",
            "  adding: runs_gan/correctedmetaclassifier_wgan_mnist/ (stored 0%)\n",
            "  adding: runs_gan/correctedmetaclassifier_wgan_mnist/events.out.tfevents.1592132987.24df03861d3b (stored 0%)\n",
            "  adding: runs_gan/correctedmetaclassifier_wgan_mnist/events.out.tfevents.1592132993.24df03861d3b (deflated 75%)\n",
            "  adding: runs_gan/correctedmetaclassifier_wgan_mnist/events.out.tfevents.1592132928.24df03861d3b (stored 0%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nl_j1r-izlAI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -r ./checkpoints/*\n",
        "!rm -r ./results/*\n",
        "!rm -r ./runs_gan/*"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQU1_CP58eyD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9b33fe68-e79a-4ce8-8a28-e44c1ac3a795"
      },
      "source": [
        "!mv correctedmetaclassifiermnist_result.zip /content/drive/\"My Drive\"/\n",
        "!mv correctedmetaclassifiermnist_checkpoint.zip /content/drive/\"My Drive\"/\n",
        "!mv correctedmetaclassifiermnist_log.zip /content/drive/\"My Drive\"/\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mv: cannot stat 'correctedmetaclassifiermnist_result.zip': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}